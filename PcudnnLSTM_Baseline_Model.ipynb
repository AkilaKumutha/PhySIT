{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_4FezhF-dilH","outputId":"9feba4b8-8019-4c99-c2e9-f72b9e0d797b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n","Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.3.0+cu121)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (4.12.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0->torchvision)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0->torchvision)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0->torchvision)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0->torchvision)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0->torchvision)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0->torchvision)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0->torchvision)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0->torchvision)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0->torchvision)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0->torchvision)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0->torchvision)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->torchvision)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0->torchvision) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0->torchvision) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n","Collecting pyDOE\n","  Downloading pyDOE-0.3.8.zip (22 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyDOE) (1.25.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyDOE) (1.11.4)\n","Building wheels for collected packages: pyDOE\n","  Building wheel for pyDOE (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyDOE: filename=pyDOE-0.3.8-py3-none-any.whl size=18168 sha256=b7d6fadbd53b356d323fd916b8b0eb5adbdd1e6a1b758109f7ac5fac62b8a122\n","  Stored in directory: /root/.cache/pip/wheels/ce/b6/d7/c6b64746dba6433c593e471e0ac3acf4f36040456d1d160d17\n","Successfully built pyDOE\n","Installing collected packages: pyDOE\n","Successfully installed pyDOE-0.3.8\n"]}],"source":["!pip install torchvision\n","!pip install pyDOE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KqmQpWxud27e"},"outputs":[],"source":["import sys\n","sys.path.insert(0, '.')\n","import argparse\n","import os\n","import torch\n","from collections import OrderedDict\n","import math\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.io\n","from scipy.interpolate import griddata\n","#from plotting import newfig, savefig\n","from mpl_toolkits.axes_grid1 import make_axes_locatable\n","import matplotlib.gridspec as gridspec\n","import torchvision.transforms as transforms\n","from torchvision.utils import save_image\n","from torch.utils.data import TensorDataset, DataLoader\n","from torchvision import datasets\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","import seaborn as sns\n","import pylab as py\n","import time\n","from pyDOE import lhs\n","import scipy\n","import warnings\n","sys.path.insert(0, '.')\n","warnings.filterwarnings('ignore')\n","np.random.seed(1234)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OAW9qOstd_HL","outputId":"216593b9-6572-47a5-c3a3-d50f9a391abc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xseWEYkAetXL"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mjIeTqaWeGRm","outputId":"35d21cd5-77d6-4efc-8599-b444748eae9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['nav_lat', 'nav_lon', 'time_centered', 'sithic'], dtype='object')\n","(2221440, 4)\n"]}],"source":["data1 = pd.read_csv(\"/content/drive/MyDrive/Dataset-Akhila/NewReq_Arc1-features.csv\")\n","df_features = pd.DataFrame(data1, columns = data1.columns)\n","#y = data1[\"sithic_value\"].values\n","X = df_features\n","X\n","X['time_centered'] = pd.to_datetime(df_features['time_centered']).astype(int)\n","print(df_features.columns)\n","print(X.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9PlElEJ0fFXl"},"outputs":[],"source":["df_features = pd.DataFrame(data1, columns = data1.columns)\n","y = data1[\"sithic\"].values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V7c7_2v_fPfz"},"outputs":[],"source":["#Set the 75/25% train/test split, set a random state to get train/test for future analysis\n","X = df_features\n","X\n","X['time_centered'] = pd.to_datetime(df_features['time_centered']).astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"_94Ir8XafRLn","outputId":"60995684-5ab3-4eca-ea95-edde5904cdae"},"outputs":[{"data":{"text/plain":["(2221440, 4)"]},"execution_count":92,"metadata":{},"output_type":"execute_result"}],"source":["X.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h1ribEkXfT53","outputId":"ac1a62e1-8fa7-41dd-d040-838b6348f0c2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-58.420696, -53.43733 , 148.11154 , ...,  72.442535,  72.458015,\n","        72.47296 ])"]},"metadata":{},"execution_count":8}],"source":["B=X[\"nav_lon\"].values\n","B"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zl9KxbZRf1le","outputId":"09eac361-e199-471a-c1d7-9609f26d1573"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["nav_lat          float64\n","nav_lon          float64\n","time_centered      int64\n","sithic           float64\n","dtype: object"]},"metadata":{},"execution_count":9}],"source":["x= data1[\"nav_lon\"].values\n","t=data1[\"time_centered\"].values\n","t\n","df_features.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vuwNzZQXfXfF","outputId":"65bb5287-e69f-4265-d4f0-719c1d474f0e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index([1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975,\n","       ...\n","       2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013],\n","      dtype='int32', length=2165904)"]},"metadata":{},"execution_count":10}],"source":["import pandas as pd\n","####Test differentiation#########\n","\n","# Define the array of timestamps\n","time_strings = t\n","\n","# Convert the strings to datetime objects\n","time_objects = pd.to_datetime(time_strings)\n","\n","# Extract the dates\n","dates = time_objects.year\n","\n","# Create a DataFrame with the year information\n","year_info = pd.DataFrame({'year': dates})\n","\n","\n","# Concatenate X DataFrame and year_info DataFrame horizontally\n","X_with_year = pd.concat([X, year_info], axis=1)\n","\n","# Create a Pandas DataFrame\n","#df_date = pd.DataFrame(data={'dates': dates})\n","\n","# Print the DataFrame\n","\n","X_with_year\n","#nan_indices = year_info['year'].isnull().sum()\n","#nan_indices\n","# Filter X for data corresponding to the year 2007\n","X_2007 = X_with_year[X_with_year['year'] == 2014]\n","data1['time_derivative'] =year_info.diff()\n","\n","data1['time_derivative']\n","# Display the updated DataFrame with extracted features for 2007\n","#print(X_2007)\n","time_objects_excluding_2007 = dates[dates != 2014]\n","time_objects_excluding_2007"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9faRKyq5gkpa","outputId":"b3af8aab-71b2-450f-a0ad-6d024b116048"},"outputs":[{"output_type":"stream","name":"stdout","text":["Minimum value of 'time_derivative': 0.0\n","Maximum value of 'time_derivative': 1.0\n"]}],"source":["min_derivative = data1['time_derivative'].min()\n","max_derivative = data1['time_derivative'].max()\n","\n","# Print the minimum and maximum values\n","print(\"Minimum value of 'time_derivative':\", min_derivative)\n","print(\"Maximum value of 'time_derivative':\", max_derivative)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"esjQ3QkWhc4x"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A_bkq9lrhOzY"},"outputs":[],"source":["# Filter X_with_year to exclude data from the year 2007\n","X_except_2007 = X_with_year[X_with_year['year'] != 2014]\n","y_except_2007 = y[X_with_year['year'] != 2014]\n","\n","# Split the data excluding 2007 into training and testing sets\n","X_train_except_2007, X_test_except_2007, y_train_except_2007, y_test_except_2007 = train_test_split(\n","    X_except_2007, y_except_2007, test_size=0.2, random_state=0\n",")\n","\n","# Filter X_with_year for data corresponding to the year 2007\n","X_2007 = X_with_year[X_with_year['year'] == 2014]\n","y_2007 = y[X_with_year['year'] == 2014]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SORcUnZjwMjl","outputId":"9e655c8c-4848-4e60-ea53-9f8b4c685688"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 4.9599170e+01 -5.8420696e+01  1.3898736e+18  4.5051612e-02\n","   2.0140000e+03]\n"," [ 5.0527065e+01 -5.3437330e+01  1.3898736e+18  3.5513207e-01\n","   2.0140000e+03]\n"," [ 5.1792973e+01  1.4811154e+02  1.3898736e+18  7.2821360e-02\n","   2.0140000e+03]\n"," ...\n"," [ 6.9997670e+01  7.2442535e+01  1.4187312e+18  1.0973377e+00\n","   2.0140000e+03]\n"," [ 6.9552220e+01  7.2458015e+01  1.4187312e+18  1.2507676e+00\n","   2.0140000e+03]\n"," [ 6.9107480e+01  7.2472960e+01  1.4187312e+18  1.4002302e+00\n","   2.0140000e+03]]\n","float64\n"]}],"source":["print(X_2007.values)\n","print(y_2007.dtype)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tjj9wjO_y6IH"},"outputs":[],"source":["X_2007=X_2007.values[:,[0,1,4]]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GEsUdWMojGmO","outputId":"b3be6804-2b4b-4c02-fba9-67068b8b60ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["(1732723, 5)\n","(433181, 5)\n","(1732723,)\n","(433181,)\n"]}],"source":["print(X_train_except_2007.shape)\n","print(X_test_except_2007.shape)\n","print(y_train_except_2007.shape)\n","print(y_test_except_2007.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0uWkebokXZmf","outputId":"f5aff6ab-c654-4d5a-e77c-a4ec2587f8bb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1732723, 3)"]},"metadata":{},"execution_count":17}],"source":["X_train=X_train_except_2007.values[:,[0,1,4]]\n","X_test=X_test_except_2007.values[:,[0,1,4]]\n","X_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e9KYcqi2oD5v"},"outputs":[],"source":["X_star=X_test\n","u_star = y_test_except_2007\n","X_u_train = X_train\n","u_train=y_train_except_2007"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DuH7sDUYqAdg"},"outputs":[],"source":["# Choose a random sample size (\n","sample_size = 10000\n","\n","# Select random indices from the entire data range (0 to length-1)\n","random_indices = np.random.choice(X_star.shape[0], size=sample_size, replace=False)\n","\n","# Select random data using the indices\n","X_star_random = X_star[random_indices]\n","u_star_random = u_star[random_indices]\n","\n","# Similarly, for training data\n","random_indices_train = np.random.choice(X_u_train.shape[0], size=sample_size, replace=False)\n","X_u_train_random = X_u_train[random_indices_train]\n","u_train_random = u_train[random_indices_train]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7gBaGj5c2sQl"},"outputs":[],"source":["import numpy as np\n","\n","# Assuming X_star might have overlapping indices with previously selected points\n","\n","# Number of elements not yet selected (might be less than 5000)\n","n_remaining = X_star.shape[0] - len(random_indices)\n","\n","# Select a random sample size (up to available elements)\n","sample_size = min(1000, n_remaining)\n","\n","# Randomly choose indices from the remaining set (if any)\n","if n_remaining > 0:\n","  # Ensure no duplicates between random_indices and chosen indices\n","  next_5000_indices = np.random.choice(np.setdiff1d(np.arange(X_star.shape[0]), random_indices), size=sample_size, replace=False)\n","  X_star_next_5000 = X_star[next_5000_indices]\n","else:\n","  # Handle the case where no elements are remaining (all might have been selected previously)\n","  print(\"No elements in X_star haven't been selected yet\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jkQUn8Zp2zLg","outputId":"b50d930b-f677-4ab6-d5b8-22257aee9e40"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([4.6202254 , 3.4149413 , 0.15174903, ..., 2.9091349 , 3.061613  ,\n","       1.5146224 ])"]},"metadata":{},"execution_count":21}],"source":["X_f_train =X_star_next_5000\n","X_u_train_random\n","u_train_random"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"y45hq3bXU735","outputId":"0d6af1f7-2553-445d-d438-8a33c5db1ae5"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1000, 3)\n","(10000, 3)\n","(10000,)\n","(10000,)\n"]}],"source":["print(X_f_train.shape)\n","print(X_u_train_random.shape)\n","print(u_train_random.shape)\n","print(u_star_random.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"bo7YvizHMShe","outputId":"68e64a86-88f3-4e6d-96f1-eae6e9164114"},"outputs":[{"name":"stdout","output_type":"stream","text":["10000\n"]}],"source":["print(len(X_u_train_random))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"48zCVuY1Mm38"},"outputs":[],"source":["# Create Network\n","import torch\n","import torch.nn as nn\n","\n","class Net(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(Net, self).__init__()\n","        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, bias=True, batch_first=True)\n","        self.lstm_1 = nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, bias=True, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim, bias=False)\n","\n","    def forward(self, x):\n","        h, state = self.lstm(x)\n","        h1, state1 = self.lstm_1(h)\n","        h2 = h1[:,-1]\n","        h3 = self.fc(h2)\n","        return h3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a78azAn8t4MR"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import scipy.io\n","from scipy.interpolate import griddata\n","from torch.utils.data import TensorDataset, DataLoader\n","import time\n","import torch.nn as nn\n","\n","def set_seed(seed_value):\n","    \"\"\"Set seed for reproducibility.\"\"\"\n","    np.random.seed(seed_value)\n","    torch.manual_seed(seed_value)\n","    torch.cuda.manual_seed_all(seed_value)\n","\n","# Set seed value for reproducibility\n","seed_value = 1234\n","set_seed(seed_value)\n","\n","class BPINN(nn.Module):\n","    def __init__(self, x_u, y_u, x_f, X_star, u_star, net, nepochs, Omega_mse, noise = 0.1):\n","        super(BPINN, self).__init__()\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        # Normalize data\n","        self.Xmean, self.Xstd = x_f.mean(0), x_f.std(0)\n","        self.x_f = (x_f - self.Xmean) / self.Xstd\n","        self.x_u = (x_u - self.Xmean) / self.Xstd\n","        self.X_star_norm = (X_star - self.Xmean) / self.Xstd\n","        self.u_star = u_star\n","\n","        #Jacobian of the PDE because of normalization\n","        self.Jacobian_X = 1 / self.Xstd[0]\n","        self.Jacobian_Y = 1 / self.Xstd[1]\n","        self.Jacobian_T = 1 / self.Xstd[2]\n","\n","        self.y_u = y_u + noise * np.std(y_u)*np.random.randn(y_u.shape[0])\n","\n","        self.net = net\n","\n","\n","        self.net_optim = torch.optim.Adam(self.net.parameters(), lr=0.001, betas = (0.9, 0.999))\n","\n","        #self.device = device\n","\n","        # numpy to tensor\n","        self.train_x_u = torch.tensor(self.x_u, requires_grad=True).float()\n","        self.train_y_u = torch.tensor(self.y_u, requires_grad=True).float()\n","        self.train_x_f = torch.tensor(self.x_f, requires_grad=True).float()\n","        self.X_star_norm = torch.tensor(self.X_star_norm, requires_grad=True).float()\n","        self.u_star = torch.tensor(self.u_star, requires_grad=True).float()\n","\n","        self.nepochs = nepochs\n","        self.Omega_mse = Omega_mse\n","\n","        self.batch_size = 100\n","        num_workers = 4\n","        shuffle = True\n","        self.train_loader = DataLoader(\n","            list(zip(self.train_x_u,self.train_y_u)), batch_size=self.batch_size, shuffle=shuffle\n","        )\n","\n","        self.val_loader = DataLoader(\n","            list(zip(self.X_star_norm,self.u_star)), batch_size=self.batch_size, shuffle=shuffle\n","        )\n","\n","\n","    def seq(self, X):\n","        #print(X.shape)\n","        num_features = X.shape[1]\n","        timesteps = 10  # Example time window (adjust as needed)\n","\n","        # Initialize output tensor for shifted features\n","        X_in = torch.zeros((len(X), timesteps, num_features), dtype=torch.float32)\n","\n","    # Iterate through features and timesteps\n","        for i in range(num_features):\n","            for j in range(timesteps):\n","                # Shift feature values and fill any missing values with the previous value\n","                shifted_feature = torch.roll(X[:, i], timesteps - j - 1, dims=0)\n","                X_in[:, j, i] = shifted_feature\n","\n","        return X_in\n","\n","    def get_residual(self, X_in):\n","\n","        X_in = self.seq(X_in)\n","        #print(X_in.shape)\n","        # physics loss for collocation/boundary points ###independent variables\n","        x = torch.tensor(X_in[:, :, 0:1], requires_grad=True).float()\n","        y = torch.tensor(X_in[:, :, 1:2], requires_grad=True).float()\n","        t = torch.tensor(X_in[:, :, 2:3], requires_grad=True).float()\n","\n","\n","        y_pred = self.net.forward(torch.cat([x, y, t], dim=2))\n","        u = y_pred[:,0:1]\n","        fh = self.phy_residual(x, y, t, u)\n","        return y_pred, fh\n","\n","    def phy_residual(self, x, y, t, u, nu=0.5,L=0.5):\n","        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n","\n","        u_t = torch.autograd.grad(\n","            u, t,\n","            grad_outputs=torch.ones_like(u),\n","            retain_graph=True,\n","            create_graph=True\n","        )[0]\n","        u_x = torch.autograd.grad(\n","            u, x,\n","            grad_outputs=torch.ones_like(u),\n","            retain_graph=True,\n","            create_graph=True\n","        )[0]\n","        u_y = torch.autograd.grad(\n","            u, y,\n","            grad_outputs=torch.ones_like(u),\n","            retain_graph=True,\n","            create_graph=True\n","        )[0]\n","\n","        fh = (self.Jacobian_T) * u_t + (self.Jacobian_X) * 1 * u_x + (self.Jacobian_Y) * 1 * u_y  + nu + L\n","        return fh\n","\n","\n","    def cal_Omega(self, adaptive_lambda, phy_loss, loss, beta=0.1): #0.1\n","        phyloss_layer = []\n","        loss_layer = []\n","        with torch.no_grad():\n","            for layer in self.net.children():\n","                if isinstance(layer, nn.Linear) or isinstance(layer, nn.LSTM):\n","                    if isinstance(layer, nn.Linear):\n","                        phyloss_layer.append(torch.abs(torch.autograd.grad(phy_loss, layer.weight, retain_graph=True)[0]).max())\n","                        loss_layer.append(torch.abs(torch.autograd.grad(loss, layer.weight, retain_graph=True)[0]).mean())\n","                    elif isinstance(layer, nn.LSTM):\n","                        for param in layer.parameters():\n","                            if param.requires_grad:\n","                                phyloss_layer.append(torch.abs(torch.autograd.grad(phy_loss, param, retain_graph=True)[0]).max())\n","                                loss_layer.append(torch.abs(torch.autograd.grad(loss, param, retain_graph=True)[0]).mean())\n","        max_grad_res = torch.stack(phyloss_layer).max()\n","        mean_grad_loss = torch.stack(loss_layer).mean()\n","        lambda_new = max_grad_res / mean_grad_loss\n","        adaptive_lambda = (1 - beta) * adaptive_lambda + beta * lambda_new\n","        return adaptive_lambda\n","\n","\n","    def train(self, patience=15):\n","        TOT_loss = np.zeros(self.nepochs)\n","        MSE_loss = np.zeros(self.nepochs)\n","        PHY_loss = np.zeros(self.nepochs)\n","        VAL_loss = np.zeros(self.nepochs)\n","\n","        epoch_times = []\n","        start_time = time.perf_counter()\n","        min_val_loss = 9999\n","        no_improvement_count = 0\n","        # Define the number of batches to accumulate gradients over\n","        accum_iter = 4  # Adjust this value based on your memory limitations\n","        for epoch in range(self.nepochs):\n","            print(\"Epoch: \", epoch)\n","            epoch_loss = 0\n","            epoch_val_loss = 0\n","\n","            for i, (x, y) in enumerate(self.train_loader):\n","                self.net_optim.zero_grad()\n","                y_pred, _ = self.get_residual(x)\n","                _, fh = self.get_residual(self.train_x_f)\n","\n","                mse_loss = torch.nn.functional.mse_loss(y, y_pred)\n","                physics_loss = torch.mean(fh**2)\n","\n","                self.Omega_mse = self.cal_Omega(self.Omega_mse, physics_loss, self.Omega_mse * mse_loss)\n","\n","                loss = self.Omega_mse * mse_loss + physics_loss\n","                print(\"Epoch: \", epoch,\"Loss: \" ,loss)\n","\n","                #loss = (self.Omega_mse * mse_loss + physics_loss)\n","\n","                loss.backward(retain_graph=True)\n","                # Update weights after 'accum_iter' or last batch\n","                if (i + 1) % accum_iter == 0 or (i + 1) == len(self.train_loader):\n","                  self.net_optim.step()\n","                  self.net_optim.zero_grad()  # Reset gradients for next iteration\n","                self.net_optim.step()\n","\n","                TOT_loss[epoch] += loss.detach().cpu().numpy()\n","                MSE_loss[epoch] += mse_loss.detach().cpu().numpy()\n","                PHY_loss[epoch] += physics_loss.detach().cpu().numpy()\n","\n","            TOT_loss[epoch] /= len(self.train_loader)\n","            MSE_loss[epoch] /= len(self.train_loader)\n","            PHY_loss[epoch] /= len(self.train_loader)\n","\n","            # Calculate validation loss\n","            #with torch.no_grad():\n","            self.net.eval()\n","            for x_val, y_val in self.val_loader:\n","                y_pred_val, _ = self.get_residual(x_val)\n","                val_mse_loss = torch.nn.functional.mse_loss(y_val, y_pred_val)\n","                epoch_val_loss += val_mse_loss.detach().cpu().numpy()\n","            VAL_loss[epoch] = epoch_val_loss / len(self.train_loader)\n","\n","            if min_val_loss > val_mse_loss**0.2:#0.5\n","                min_val_loss = val_mse_loss**0.2\n","\n","                no_improvement_count = 0\n","            else:\n","                no_improvement_count += 1\n","\n","            if no_improvement_count == patience:\n","                break\n","\n","            if epoch % 10 == 0:\n","                print(\n","                    \"[Epoch %d/%d] [Phy loss: %f] [Total loss: %f] [mse_loss: %f] [val_loss: %f]\"\n","                    % (epoch, self.nepochs, PHY_loss[epoch], TOT_loss[epoch], MSE_loss[epoch], VAL_loss[epoch])\n","                )\n","\n","        total_training_time = time.perf_counter() - start_time\n","        print(\"Total Training Time: {} seconds\".format(total_training_time))\n","        plt.figure(figsize=(6, 6))\n","        plt.plot(MSE_loss, label='MSE loss')\n","        plt.plot(VAL_loss, label='VAL loss')\n","        plt.ylim(0, 9)\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Loss')\n","        plt.legend()\n","        plt.grid(False)\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vaK2b4Sk4UIr"},"outputs":[],"source":["num_epochs = 10\n","Omega_mse = 1\n","noise = 0.01"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6EnRhAdFS_18"},"outputs":[],"source":["net=Net(input_dim=3, hidden_dim=64, output_dim=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7LAton1TS_19","outputId":"71387756-0405-4ec4-bc59-9d44301db845"},"outputs":[{"output_type":"stream","name":"stdout","text":["Net(\n","  (lstm): LSTM(3, 16, batch_first=True)\n","  (lstm_1): LSTM(16, 16, batch_first=True)\n","  (fc): Linear(in_features=16, out_features=1, bias=False)\n",")\n"]}],"source":["print(net)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_mcmx4-p34z8"},"outputs":[],"source":["burgers = BPINN(X_u_train_random, u_train_random, X_f_train, X_star_random, u_star_random, net, num_epochs, Omega_mse, noise)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"9MHHfv-H5aG1","outputId":"cb37e099-eb99-4280-a43e-90d020fa86f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch:  0\n","Epoch:  0 Loss:  tensor(7.6855, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(8.5412, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(7.4878, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(6.1517, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(6.0015, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(5.0625, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(4.8914, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(5.3968, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(4.0619, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(3.8189, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(3.9260, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(3.1087, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(2.6561, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(2.9736, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(2.6965, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(2.5490, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(2.5611, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(2.2545, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(2.6150, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(2.1098, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(2.2101, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(2.0622, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.9218, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.9009, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.9668, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(2.0433, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.8371, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.7557, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.8955, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.9664, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.9591, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.8705, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.9963, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.8586, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.8855, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.9351, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.7847, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.8838, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.7621, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(2.0757, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.6932, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.8146, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.7659, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.9075, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.8030, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.8565, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.6956, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.7898, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.7088, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.9545, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.9469, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.5966, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.7067, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.7733, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.6608, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.6758, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.5889, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.6043, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.5848, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.5778, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.6941, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.5522, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.6196, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.4208, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.4429, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.5622, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.3903, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.4003, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.4558, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.4613, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.4208, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.4977, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.3859, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.3541, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.3517, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.3144, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.4075, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.3947, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.3246, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.3560, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.3918, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.4132, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.4462, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.3277, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.4712, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.4195, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.4176, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.4286, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.4106, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.3960, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.3721, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.5426, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.5429, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.5335, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.5516, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.4518, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.4658, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.6750, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.4914, grad_fn=<AddBackward0>)\n","Epoch:  0 Loss:  tensor(1.6732, grad_fn=<AddBackward0>)\n","[Epoch 0/10] [Phy loss: 0.998746] [Total loss: 2.180553] [mse_loss: 4.909910] [val_loss: 2.158839]\n","Epoch:  1\n","Epoch:  1 Loss:  tensor(1.5345, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.6372, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.8834, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.7008, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.6463, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.5700, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.5918, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.6238, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.5349, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.8175, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.5248, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.4781, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.4194, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.4673, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.6097, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.4854, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.4317, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.9433, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(3.3539, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(3.5062, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.9421, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(3.1772, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.2417, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.2779, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.1868, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.2152, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.2400, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.9923, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.9031, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.0579, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.1288, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.6300, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.6087, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.6393, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.6798, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.6391, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.5750, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.9367, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.9636, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.7658, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.9067, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.6467, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.7640, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.8299, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.8998, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.6994, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.6713, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.6718, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.5403, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.7847, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.6868, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.7701, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.8859, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.7782, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.1276, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.1120, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.9517, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.0752, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.2929, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.9506, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.7434, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.8261, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.8970, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.9362, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.1902, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.9388, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.3254, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.6968, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.6064, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.5123, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.2468, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.5528, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.3877, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.0681, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.1071, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.9481, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.0110, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.8857, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.8374, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.0338, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.2929, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.9830, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.0427, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.8918, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.1897, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.7837, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(2.1205, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.7860, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.8302, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.6694, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.8261, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.9211, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.7965, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.7783, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.6896, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.7467, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.9081, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.9793, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.8279, grad_fn=<AddBackward0>)\n","Epoch:  1 Loss:  tensor(1.7427, grad_fn=<AddBackward0>)\n","Epoch:  2\n","Epoch:  2 Loss:  tensor(1.8760, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(1.7451, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.2237, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.1350, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(1.7517, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.0502, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.3568, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(1.6308, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.0309, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.6584, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.5839, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.4425, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.0048, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.1069, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(1.8841, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.4380, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.4053, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(1.9639, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.0993, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.3794, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(1.8927, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(1.9178, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.0019, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(1.8622, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(1.9027, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(1.8988, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(1.7514, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.0796, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(1.9547, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(1.6895, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(1.8393, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.4589, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.5864, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.6726, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(3.1771, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(3.0386, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.8388, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(3.1493, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.7389, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.7251, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.3290, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.3161, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.3019, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.0799, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.2743, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.4331, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.0297, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(1.8530, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.1007, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.0227, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.4448, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.1095, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.2518, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(1.9952, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.1094, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.0793, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.0814, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.1755, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.2625, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(1.7865, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.3030, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.7455, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.8586, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.8236, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.7070, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.7406, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.4835, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.7944, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.5365, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.5555, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.3507, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.3728, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.1961, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.5383, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.3040, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.2847, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.3026, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.4571, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.1840, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(1.9427, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(1.8846, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(1.8460, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(1.9135, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.1747, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(1.8408, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.0013, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(1.9701, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.0812, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.2332, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(1.9450, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.5848, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.1251, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.6597, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.8438, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.2710, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.9085, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.7821, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(3.1253, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.3484, grad_fn=<AddBackward0>)\n","Epoch:  2 Loss:  tensor(2.8174, grad_fn=<AddBackward0>)\n","Epoch:  3\n","Epoch:  3 Loss:  tensor(2.9694, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.2533, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.4672, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.5659, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.3301, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.5430, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.4536, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.4286, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.2422, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.2581, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.2052, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.0898, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.3833, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.1446, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.1565, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.3128, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.0375, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.0694, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.1248, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.4138, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.2954, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.1950, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.3763, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.1972, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.4950, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.3180, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.2159, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.2107, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.2509, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.0277, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.1736, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.0890, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(1.9847, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.1810, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.2181, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.0667, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.0999, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.0138, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.1721, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.0122, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.1882, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.3578, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.1969, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.0296, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.3980, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.0615, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.2334, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.3520, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.0089, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(1.9519, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(1.9736, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(1.9468, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(1.8589, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.1918, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.2149, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.2791, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.0111, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.1171, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.1410, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.3387, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.2960, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.4884, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.2365, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.1279, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(5.3295, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(5.7570, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(4.8032, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(4.9153, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(3.9150, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(3.3642, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(3.4580, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(4.3076, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(3.0389, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(3.1468, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(3.5403, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.6295, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.6811, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.7661, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.6002, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.9448, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.2254, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.2308, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.3503, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.1154, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.4067, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.2142, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.2553, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(1.9851, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.1330, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.1738, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.2092, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.0167, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.7554, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.1884, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.2424, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.5809, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.6003, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.2309, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(3.0355, grad_fn=<AddBackward0>)\n","Epoch:  3 Loss:  tensor(2.1120, grad_fn=<AddBackward0>)\n","Epoch:  4\n","Epoch:  4 Loss:  tensor(2.4013, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.2435, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.5482, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.4863, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.2704, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.6112, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.5483, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(3.6412, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(3.2787, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.6720, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(3.1433, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(3.4592, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.9853, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(3.0925, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(3.6495, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.7221, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(3.1024, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.7806, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(3.2940, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.6705, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.4547, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.4660, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.4854, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.6053, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.3557, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.3208, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.1432, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.2825, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.3189, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.1188, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(1.8597, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.0603, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.2107, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.2709, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.4192, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.3436, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(1.9832, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.1231, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.2206, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.1317, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.0065, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.3510, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.1711, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(1.9562, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.3445, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.4272, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.3622, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.5877, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(1.8272, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.1200, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(1.9775, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.1540, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.0532, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(1.9109, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.0168, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(1.9328, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(1.9759, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.1033, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.0663, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.7924, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.0752, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(3.2844, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.4732, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.7491, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.8216, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.6570, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(3.2646, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(3.0894, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.9032, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.7187, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.8651, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.4789, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.6298, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.6036, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.5097, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.2383, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(3.1597, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.8028, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.6237, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.8254, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.4508, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.2098, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.4625, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.7943, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.6006, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.4066, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.0026, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.1233, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(3.4674, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(3.1124, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.8678, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(3.0237, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.9086, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.8823, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.6313, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(3.9356, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(3.7992, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(3.9670, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(3.2731, grad_fn=<AddBackward0>)\n","Epoch:  4 Loss:  tensor(2.6452, grad_fn=<AddBackward0>)\n","Epoch:  5\n","Epoch:  5 Loss:  tensor(3.3531, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.5601, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.8169, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.7841, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.4495, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.1658, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.3470, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.2088, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.3630, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.3027, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.2034, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.3644, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.2607, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.2033, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(4.0549, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(3.7898, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(3.2789, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(3.2323, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(3.0384, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.5145, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.7883, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.7332, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.3903, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.2528, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.3442, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(10.0990, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(7.4570, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(7.7591, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(6.4222, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(6.4556, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(4.6130, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(4.1272, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(4.9609, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(3.9753, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(3.7987, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(3.9461, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(3.1527, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(3.6235, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.6058, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(3.0626, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(3.0607, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.9133, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.9647, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.6884, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.5928, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.4793, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.0767, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.2789, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.1142, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.0350, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(1.9290, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.0015, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.3924, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(12.9976, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(13.3625, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(12.3682, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(11.3236, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(7.7399, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(8.8054, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(8.7640, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(5.9410, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(7.4405, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(6.5689, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(4.6014, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(4.9358, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(3.7903, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(5.1302, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(3.6019, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(4.1463, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(3.6644, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(3.4878, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(3.1380, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(3.3086, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.1315, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.6205, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.6323, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.2872, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.4480, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.1877, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.0147, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.1767, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.1703, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.0423, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(1.8641, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(1.7251, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(1.6374, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(1.9035, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(1.8023, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(1.9736, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.0217, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(1.9884, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(2.1288, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(1.7274, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(1.8947, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(1.9175, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(1.9366, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(1.8289, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(1.8351, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(1.7140, grad_fn=<AddBackward0>)\n","Epoch:  5 Loss:  tensor(1.9645, grad_fn=<AddBackward0>)\n","Epoch:  6\n","Epoch:  6 Loss:  tensor(2.2899, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.2779, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(1.7825, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.5708, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(1.9604, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.1447, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(1.8502, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(1.7533, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(1.6786, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.0278, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(3.9349, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.8498, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(3.3955, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.7370, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.9497, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.9320, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(3.2989, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.6042, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.2785, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.2805, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.1926, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.0184, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.1074, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(1.8769, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(1.7470, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(1.8141, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(3.4001, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(3.1949, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(3.0140, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.8470, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(3.3904, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.7959, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.4923, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.5678, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.3473, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.3079, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.2828, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.7861, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.7722, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.6930, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.5029, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(4.2459, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(3.2983, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(3.2154, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.8943, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.7489, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.9868, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.0477, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.5940, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.7115, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.6465, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.5224, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.3442, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.7792, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.5383, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.7745, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.4922, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.4584, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.7306, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.2889, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(3.0724, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.5619, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.6768, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.3433, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.5552, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.7503, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.1532, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.3037, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.6779, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.1011, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.6940, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.9376, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.0933, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.2726, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(3.2775, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(3.1775, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(3.7072, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(3.4203, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.9280, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.8204, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.6724, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.3137, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.7341, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(3.1632, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.8099, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(3.0871, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.8921, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(3.2352, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.7622, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.5595, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.5054, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.3032, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.2996, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.0348, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(1.9859, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.1574, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.4796, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.3249, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(2.0048, grad_fn=<AddBackward0>)\n","Epoch:  6 Loss:  tensor(1.9444, grad_fn=<AddBackward0>)\n","Epoch:  7\n","Epoch:  7 Loss:  tensor(1.8592, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.1000, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.8753, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.2410, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.5124, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.8824, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.0734, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.7340, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.2911, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.2741, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.0373, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.2754, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.2589, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.2357, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.2658, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.1275, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.6332, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.4460, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.1820, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.4538, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.2779, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.2539, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.4175, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.2556, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.5470, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.1913, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.3183, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(1.8992, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.5966, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(5.1316, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.7017, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.8018, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.4765, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.3404, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.2397, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.7738, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.5987, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.8580, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.3783, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.1910, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.4386, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.3516, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.0825, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.6487, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.2962, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.3799, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.5927, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.5740, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.3495, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.6963, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.5511, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(4.5278, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.4675, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.3780, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.4580, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(5.6144, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(4.6826, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(4.3675, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(4.0455, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.6478, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.7241, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.4315, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.2546, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.8816, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.2880, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.0239, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.6788, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.0408, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.7243, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.1855, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.5498, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.3323, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.4324, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(1.8543, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.0469, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.0206, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(1.9672, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.1973, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(1.9312, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(1.8534, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(1.9543, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(1.7860, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(1.9023, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(1.7363, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.0890, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.4496, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.5811, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.5509, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.3135, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(5.0134, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(4.2127, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.7380, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.8490, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.6802, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.6102, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.4579, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.7912, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(3.1615, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.6391, grad_fn=<AddBackward0>)\n","Epoch:  7 Loss:  tensor(2.6251, grad_fn=<AddBackward0>)\n","Epoch:  8\n","Epoch:  8 Loss:  tensor(2.6394, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.5021, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.2348, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.2701, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.5758, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.2315, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.3192, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(1.9568, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(3.1822, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.4201, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.6189, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.9884, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(3.7215, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.4956, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.6901, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.5054, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.6897, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.6371, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.3600, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.1260, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.1848, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(1.8197, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.0267, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.1604, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(1.9382, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.8776, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.5470, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.6905, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.8358, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.9858, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.9569, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.8424, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.6081, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.4966, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.1930, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.6411, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.3552, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.5884, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(1.8734, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.3075, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.2479, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.6268, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.0987, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.8742, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(3.1817, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(3.5610, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.5329, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.8190, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.7732, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.5181, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.5204, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.7629, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.6729, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.6454, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.3292, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.6514, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.2604, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.5917, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.8376, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.3135, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.6984, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.2304, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.6064, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.4534, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(1.8149, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.0013, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.0181, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.1598, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(1.8690, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(1.9666, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(1.9743, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.1005, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.2840, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(1.9069, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.4847, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.7098, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.4819, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.1073, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.2982, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.1143, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(1.9401, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(1.9100, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(1.8480, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(1.9172, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(1.7671, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(1.9538, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.1908, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.0961, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.0410, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.4686, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.4877, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.4567, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.5419, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.9229, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.2675, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.6950, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.7463, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(3.1234, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(2.4483, grad_fn=<AddBackward0>)\n","Epoch:  8 Loss:  tensor(4.4440, grad_fn=<AddBackward0>)\n","Epoch:  9\n","Epoch:  9 Loss:  tensor(3.5139, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(5.2003, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(4.2527, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(4.9507, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(4.3816, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(3.7493, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(3.1791, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(3.6828, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(3.0359, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(3.8626, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(3.1235, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.6133, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.8386, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.8902, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.4549, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.4957, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.0258, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.2910, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.6378, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.3193, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.1590, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.4487, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.9124, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.6217, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.5382, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(1.9048, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.0221, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.0486, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(3.2045, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.6186, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(3.1492, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.1610, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.6586, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(3.0829, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(3.6005, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(3.4191, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(3.2390, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.7787, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.7376, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(3.3625, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.5327, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.7202, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.6330, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.1608, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.4794, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.2810, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.3143, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.1832, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.4355, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.7771, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.3387, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.4465, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.4184, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.3605, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.2505, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.0763, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.3344, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.2234, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.3098, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.0748, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(1.9092, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.2683, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(1.8812, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(1.8346, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.0222, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.7616, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.1551, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.7118, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.2595, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.7438, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.6444, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.7291, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(3.1107, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.7092, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.6999, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.6101, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.2420, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.3100, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.1298, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.3740, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(1.8516, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.1932, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.2098, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.2855, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.5116, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.1107, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(1.9346, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.4638, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.7680, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.9050, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.7795, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.3091, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.1117, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.4798, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.1069, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.0742, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.1868, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.2544, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.0965, grad_fn=<AddBackward0>)\n","Epoch:  9 Loss:  tensor(2.3967, grad_fn=<AddBackward0>)\n","Total Training Time: 1362.212952058001 seconds\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 600x600 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["burgers.train()"]},{"cell_type":"markdown","metadata":{"id":"iasV2N1bvt3D"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rHz055TYl0m7"},"outputs":[],"source":["import torch\n","import random\n","import numpy as np\n","\n","# Set random seed for reproducibility\n","random_seed = 42\n","torch.manual_seed(random_seed)\n","np.random.seed(random_seed)\n","random.seed(random_seed)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c-UxwkxPuvSo"},"outputs":[],"source":["def smape(y_true, y_pred):\n","    \"\"\"\n","    Calculate Symmetric Mean Absolute Percentage Error (SMAPE).\n","    \"\"\"\n","    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n","    diff = np.abs(y_true - y_pred) / denominator\n","    diff[denominator == 0] = 0.0  # Handle division by zero\n","    return np.mean(diff)*100/ len(y_true)\n","\n","def mape(y_true, y_pred):\n","    \"\"\"\n","    Calculate Mean Absolute Percentage Error (MAPE).\n","    \"\"\"\n","    diff = np.abs(y_true - y_pred) / np.abs(y_true)\n","    diff[np.isnan(diff)] = 0.0  # Handle NaN values\n","    diff[np.isinf(diff)] = 0.0  # Handle infinite values\n","    return np.mean(diff) * 100 / len(y_true)\n","\n","def theil_coefficient(y_true, y_pred):\n","    \"\"\"\n","    Calculate Theil's Inequality Coefficient.\n","    \"\"\"\n","    n = len(y_true)\n","\n","    # Calculate three components\n","    mean_true = np.mean(y_true)\n","    mean_pred = np.mean(y_pred)\n","    diff_true = y_true - mean_true\n","    diff_pred = y_pred - mean_pred\n","\n","    # Calculate Theil's Inequality Coefficient\n","    num = np.sqrt(np.mean(diff_true**2)) + np.sqrt(np.mean(diff_pred**2))\n","    denom = np.sqrt(np.mean(y_true**2)) + np.sqrt(np.mean(y_pred**2))\n","\n","    return num / denom"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ZL87oEbpToC","outputId":"ea81f3d3-5cce-418e-c0d1-1a9d6c856e86"},"outputs":[{"output_type":"stream","name":"stdout","text":["Validation MSE for 2007: 2.239731028899207\n","Validation SMAPE for 2007: 1.1905111144110847\n","Validation MAPE for 2007: 3.5606994897344606\n","Validation Theil coefficient for 2007: 0.30043074\n"]}],"source":["batch_size = 64  # You can adjust this value as needed\n","\n","# Convert X_2007 and y_2007 to tensors\n","X_2007_tensor = torch.tensor(X_2007, dtype=torch.float32)\n","y_2007_tensor = torch.tensor(y_2007, dtype=torch.float32)\n","\n","# Calculate the total number of batches\n","num_samples = X_2007_tensor.size(0)\n","num_batches = (num_samples + batch_size - 1) // batch_size\n","\n","# Initialize lists to store predictions and MSE losses\n","predictions = []\n","mse_losses = []\n","smape_values = []\n","mape_values = []\n","theil_coefficients = []\n","\n","# Iterate over batches\n","for i in range(num_batches):\n","    # Calculate start and end indices for the current batch\n","    start_idx = i * batch_size\n","    end_idx = min((i + 1) * batch_size, num_samples)\n","\n","    # Extract the current batch from X_2007_tensor and y_2007_tensor\n","    X_batch = X_2007_tensor[start_idx:end_idx]\n","    y_batch = y_2007_tensor[start_idx:end_idx]\n","\n","    # Pass X_batch through the model to obtain predictions\n","    u_pred_batch, _ = burgers.get_residual(X_batch)\n","    predictions.append(u_pred_batch)\n","\n","    # If you have ground truth data for 2007, calculate MSE loss for the current batch\n","    if y_batch is not None:\n","        mse_loss = torch.nn.functional.mse_loss(y_batch, u_pred_batch)\n","        mse_losses.append(mse_loss.item())\n","\n","                # Calculate SMAPE\n","        smape_value = smape(y_batch.detach().numpy(), u_pred_batch.detach().numpy())\n","        smape_values.append(smape_value)\n","        mape_value = mape(y_batch.detach().numpy(), u_pred_batch.detach().numpy())\n","        mape_values.append(mape_value)\n","\n","\n","        # Calculate Theil coefficient\n","        theil_coefficient_value = theil_coefficient(y_batch.detach().numpy(), u_pred_batch.detach().numpy())\n","        theil_coefficients.append(theil_coefficient_value)\n","\n","# Concatenate predictions and calculate overall MSE loss if applicable\n","u_pred_2007 = torch.cat(predictions, dim=0)\n","val_mse_loss = None\n","if y_2007_tensor is not None:\n","    val_mse_loss = sum(mse_losses) / len(mse_losses)\n","    print(\"Validation MSE for 2007:\", val_mse_loss)\n","    print(\"Validation SMAPE for 2007:\", np.mean(smape_values))\n","    print(\"Validation MAPE for 2007:\", np.mean(mape_values))\n","    print(\"Validation Theil coefficient for 2007:\", np.mean(theil_coefficients))\n"]},{"cell_type":"markdown","metadata":{"id":"C5tUCiu0aYXD"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z1GhRXE0k7Bv"},"outputs":[],"source":["u_pred_2007.shape\n","print(torch.min(u_pred_2007))\n","print(torch.max(u_pred_2007))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i4sBnOj-k_35"},"outputs":[],"source":["import torch\n","\n","# Reshape the tensor to shape [12, 55536 / 12]\n","reshaped_tensor = u_pred_2007.view(12, -1)\n","\n","# Find the mean along the first dimension\n","mean_dim1 = torch.mean(reshaped_tensor, dim=1)\n","\n","# Print the mean along the first dimension\n","print(mean_dim1.shape)\n","import pandas as pd\n","# Detach the tensor and convert it to a NumPy array\n","mean_dim1_np = mean_dim1.detach().numpy()\n","\n","# Convert the NumPy array to a pandas DataFrame\n","df = pd.DataFrame(mean_dim1_np)\n","# Write the DataFrame to a CSV file\n","df.to_csv('phy-mean_values.csv', index=False, header=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pbYRUXjk1bE9"},"outputs":[],"source":["print(X_2007.shape)\n","print(y_2007.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VUSdMLDi6bxd"},"outputs":[],"source":["print(u_pred_2007.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"olWhvCJ-cd6z"},"outputs":[],"source":["!pwd\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}